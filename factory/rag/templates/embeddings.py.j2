import os
from typing import List, Optional
import openai
from azure.identity import DefaultAzureCredential
from config import get_env_var


class EmbeddingService:
    """Service for generating embeddings using Azure OpenAI"""
    
    def __init__(self, embedding_config):
        self.config = embedding_config
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize the OpenAI client based on configuration"""
        # Use Azure OpenAI by default
        azure_endpoint = get_env_var('AZURE_OPENAI_ENDPOINT')
        api_key = get_env_var('AZURE_OPENAI_API_KEY')
        api_version = get_env_var('AZURE_OPENAI_API_VERSION', '2024-02-15-preview')
        
        if azure_endpoint and api_key:
            # Use Azure OpenAI
            self.client = openai.AzureOpenAI(
                azure_endpoint=azure_endpoint,
                api_key=api_key,
                api_version=api_version
            )
        else:
            # Use standard OpenAI
            openai_key = get_env_var('OPENAI_API_KEY')
            if not openai_key:
                raise ValueError("Either AZURE_OPENAI_ENDPOINT/AZURE_OPENAI_API_KEY or OPENAI_API_KEY must be set")
            self.client = openai.OpenAI(api_key=openai_key)
    
    async def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for a single text"""
        try:
            response = self.client.embeddings.create(
                model=self.config.model,
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"Error generating embedding: {str(e)}")
            raise
    
    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts (batch processing)"""
        embeddings = []
        
        # Process in batches to avoid rate limits
        batch_size = 100
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            try:
                response = self.client.embeddings.create(
                    model=self.config.model,
                    input=batch_texts
                )
                
                batch_embeddings = [item.embedding for item in response.data]
                embeddings.extend(batch_embeddings)
                
            except Exception as e:
                print(f"Error generating batch embeddings: {str(e)}")
                # Fallback to individual processing
                for text in batch_texts:
                    try:
                        embedding = await self.generate_embedding(text)
                        embeddings.append(embedding)
                    except Exception as individual_error:
                        print(f"Error generating embedding for text: {str(individual_error)}")
                        embeddings.append([])  # Empty embedding as fallback
        
        return embeddings
    
    def get_embedding_dimension(self) -> int:
        """Get the dimension of the embedding model"""
        # Known dimensions for common models
        model_dimensions = {
            "text-embedding-ada-002": 1536,
            "text-embedding-3-small": 1536,
            "text-embedding-3-large": 3072,
        }
        
        if self.config.dimension:
            return self.config.dimension
        
        return model_dimensions.get(self.config.model, 1536)  # Default to 1536
    
    async def embed_document_chunks(self, chunks: List[str]) -> List[dict]:
        """Embed document chunks and return with metadata"""
        if not chunks:
            return []
        
        # Generate embeddings
        embeddings = await self.generate_embeddings(chunks)
        
        # Create chunk objects with embeddings
        embedded_chunks = []
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            embedded_chunks.append({
                'chunk_id': f"chunk_{i}",
                'content': chunk,
                'embedding': embedding,
                'chunk_index': i,
                'token_count': self._count_tokens(chunk)
            })
        
        return embedded_chunks
    
    def _count_tokens(self, text: str) -> int:
        """Count tokens in text (approximate)"""
        try:
            import tiktoken
            encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
            return len(encoding.encode(text))
        except:
            # Fallback: approximate tokens (4 characters per token)
            return len(text) // 4
    
    async def health_check(self) -> bool:
        """Check if the embedding service is healthy"""
        try:
            # Try to generate a simple embedding
            test_embedding = await self.generate_embedding("test")
            return len(test_embedding) > 0
        except Exception as e:
            print(f"Embedding service health check failed: {str(e)}")
            return False
